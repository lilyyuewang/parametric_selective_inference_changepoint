================================================================================
DYNAMIC PROGRAMMING IN CHANGEPOINT DETECTION: DETAILED EXPLANATION
================================================================================

OVERVIEW
--------

This project uses Dynamic Programming (DP) to solve the optimal changepoint 
detection problem with selective inference. The DP algorithm finds the 
segmentation of a time series that minimizes the sum of squared errors (SSE) 
while simultaneously constructing constraint matrices needed for valid 
statistical inference.


WHAT JOB DOES DYNAMIC PROGRAMMING PERFORM?
------------------------------------------

PRIMARY OBJECTIVE

The DP algorithm solves the following optimization problem:

Given:
- A time series data vector x = [x_0, x_1, ..., x_{n-1}] of length n
- A fixed number of segments K (changepoints = K-1)

Find:
- The optimal positions of K-1 changepoints that minimize the total sum of 
  squared errors (SSE) across all segments
- All constraint matrices needed for selective inference to compute valid 
  p-values


MATHEMATICAL FORMULATION

The optimization problem is:

    minimize: sum(k=1 to K) SSE(segment_k)
    subject to: K segments, non-overlapping, covering all data points

Where SSE for a segment from index j to i is:

    SSE(j, i) = sum(t=j to i) (x_t - mu_ji)^2

And mu_ji is the mean of segment [j, i]:

    mu_ji = (sum(t=j to i) x_t) / (i - j + 1)


WHY DYNAMIC PROGRAMMING?

Without DP, we would need to evaluate all possible combinations of changepoint 
positions:
- For K segments in n data points, there are C(n-1, K-1) possible segmentations
- This grows exponentially with n and K
- DP reduces this to O(K x n^2) time complexity


WHAT IS A CHANGEPOINT?
----------------------

A changepoint is a position in the time series where the statistical properties 
of the data change. Most commonly, this refers to a change in the mean value.

Example:
  Data: [1, 1, 1, 5, 5, 5]
  Indices: 0  1  2  3  4  5
  
  There is a changepoint at position 3, where:
  - Segment 1 (indices 0-2): mean = 1
  - Segment 2 (indices 3-5): mean = 5

In general:
- A changepoint occurs BETWEEN data points (not at a data point)
- If we have n data points, there are n-1 possible positions for changepoints
- These positions are: between (0,1), between (1,2), ..., between (n-2, n-1)


WHY C(n-1, K-1) POSSIBLE SEGMENTATIONS?
---------------------------------------

To understand this, let's break down the combinatorics:

1. We have n data points (indices 0 to n-1)
2. We want to divide them into K segments
3. To create K segments, we need K-1 changepoints

4. Changepoints can be placed between data points:
   - Position 1: between data points 0 and 1
   - Position 2: between data points 1 and 2
   - ...
   - Position n-1: between data points n-2 and n-1
   - Total: n-1 possible changepoint positions

5. We need to choose K-1 positions out of n-1 possible positions
   - This is a combination problem: "choose K-1 from n-1"
   - Formula: C(n-1, K-1) = (n-1)! / ((K-1)! × (n-K)!)

Example 1: n=6, K=2 (1 changepoint)
  - Data points: [0, 1, 2, 3, 4, 5]
  - Possible changepoint positions: {1, 2, 3, 4, 5} (5 positions)
  - We need to choose 1 position: C(5, 1) = 5 possible segmentations
  - Options: changepoint at 1, 2, 3, 4, or 5
    * At 1: segments [0] and [1-5]
    * At 2: segments [0-1] and [2-5]
    * At 3: segments [0-2] and [3-5]
    * At 4: segments [0-3] and [4-5]
    * At 5: segments [0-4] and [5]

Example 2: n=6, K=3 (2 changepoints)
  - Possible changepoint positions: {1, 2, 3, 4, 5} (5 positions)
  - We need to choose 2 positions: C(5, 2) = 10 possible segmentations
  - Options: (1,2), (1,3), (1,4), (1,5), (2,3), (2,4), (2,5), (3,4), (3,5), (4,5)
  - Example: changepoints at (2, 4) gives segments [0-1], [2-3], [4-5]

Why this grows exponentially:
  - C(n-1, K-1) grows very quickly with n and K
  - For n=100, K=10: C(99, 9) ≈ 1.7 × 10^12 (over 1 trillion combinations!)
  - Evaluating all combinations becomes computationally infeasible
  - Dynamic Programming avoids this by reusing subproblem solutions


DYNAMIC PROGRAMMING ALGORITHM STRUCTURE
---------------------------------------

The algorithm consists of two main phases:

1. Forward DP Phase: Fill DP tables to find optimal segmentation
2. Backtracking Phase: Recover the optimal changepoint positions


================================================================================
DETAILED VARIABLE EXPLANATIONS
================================================================================

CORE DATA STRUCTURES
---------------------

1. data (input parameter)
   Type: numpy.ndarray, shape (n,)
   Description: The input time series data
   Example: [1.2, 1.5, 1.3, 3.1, 3.2, 2.9, 5.1, 5.0]
   Usage: Raw observations that we want to segment

2. n (derived)
   Type: int
   Description: Length of the data vector
   Formula: n = len(data)
   Example: If data has 100 elements, n = 100

3. K or n_segments (input parameter)
   Type: int
   Description: Number of segments to partition the data into
   Note: Number of changepoints = K - 1
   Example: K = 3 means 2 changepoints dividing data into 3 segments


DP TABLE VARIABLES
------------------

4. S[k][i] - Optimal Cost Table
   Type: numpy.ndarray, shape (K, n), dtype float64
   Description: S[k][i] stores the minimum total SSE achievable when:
                - Using exactly k+1 segments (k changepoints)
                - Ending at position i (inclusive)
   Mathematical Meaning:
     S[k][i] = min_{j_1, j_2, ..., j_k} sum(m=1 to k+1) SSE(segment_m)
     where segments cover positions [0, i]
   Initialization:
     S[0][i] = SSE(0, i) for all i (single segment from start to i)
   Recurrence Relation:
     S[k][i] = min_{j=k to i-1} { S[k-1][j-1] + SSE(j, i) }
   Example:
     S[1][5] = minimum SSE using 2 segments ending at position 5
     S[2][10] = minimum SSE using 3 segments ending at position 10

5. J[k][i] - Backtracking Table
   Type: numpy.ndarray, shape (K, n), dtype int64
   Description: J[k][i] stores the optimal starting position of the last 
                segment when using k+1 segments ending at position i
   Mathematical Meaning:
     J[k][i] = argmin_{j=k to i-1} { S[k-1][j-1] + SSE(j, i) }
   Usage: Used in backtracking to recover optimal changepoint positions
   Example:
     If J[1][5] = 3, then optimal segmentation ending at 5 uses segments 
     [0,2] and [3,5]. Changepoint is at position 3

   DETAILED EXPLANATION OF THE BACKTRACKING TABLE:
   -----------------------------------------------
   
   Key Principle: Segments must be NON-OVERLAPPING and CONTIGUOUS
   
   Understanding J[k][i] = j:
   - J[k][i] stores the STARTING position of the last segment
   - The last segment is [j, i] (inclusive on both ends)
   - The previous segments must END at position j-1 (one position before j)
   - This ensures no overlap: previous segments cover [0, j-1], last segment covers [j, i]
   
   The Recurrence Relation Reveals This:
   -------------------------------------
   When computing S[k][i], we evaluate:
     SSQ_j = S[k-1][j-1] + SSE(j, i)
   
   Breaking this down:
   - S[k-1][j-1]: optimal cost for k segments ending at position j-1
     * This covers positions [0, j-1]
   - SSE(j, i): cost of segment from j to i
     * This covers positions [j, i]
   - Together: [0, j-1] + [j, i] = [0, i] with no overlap
   
   Concrete Example: J[1][5] = 3
   ------------------------------
   Data indices:  0  1  2  3  4  5
   
   J[1][5] = 3 means:
   - Last segment starts at 3: [3, 5] = {3, 4, 5}
   - Previous segment ends at 2: [0, 2] = {0, 1, 2}
   - Result: [0,2] and [3,5] ✓ (no overlap, contiguous)
   
   If it were [0,3] and [3,5]:
   - First segment: [0, 3] = {0, 1, 2, 3}
   - Last segment: [3, 5] = {3, 4, 5}
   - Position 3 appears in BOTH segments ✗ (overlap!)
   
   Backtracking Process:
   --------------------
   When backtracking (see Phase 2 in algorithm flow):
   1. Start with segment_right = n-1 (last position)
   2. segment_left = J[k][segment_right] (start of last segment)
   3. Last segment: [segment_left, segment_right]
   4. Move to previous segment: segment_right = segment_left - 1
   5. Repeat until all segments recovered
   
   Example walkthrough for J[1][5] = 3:
   - segment_right = 5
   - segment_left = J[1][5] = 3
   - Last segment: [3, 5]
   - Previous segment: segment_right = 3 - 1 = 2
   - First segment: [0, 2] (since we start at 0)
   - Final: [0,2] and [3,5]


PRECOMPUTED SUM ARRAYS (for efficiency)
----------------------------------------

6. sum_x[i] - Cumulative Sum
   Type: numpy.ndarray, shape (n,), dtype float64
   Description: Cumulative sum of data up to position i
   Formula:
     sum_x[i] = sum(t=0 to i) (x_t - shift)
   Initialization:
     sum_x[0] = x[0] - shift
     sum_x[i] = sum_x[i-1] + x[i] - shift for i > 0
   Purpose: Enables O(1) computation of segment means
   Example:
     If data = [1, 2, 3, 4] and shift = 0
     sum_x = [1, 3, 6, 10]
   Usage: To compute mean of segment [j, i]:
     mu_ji = (sum_x[i] - sum_x[j-1]) / (i - j + 1)  if j > 0
     mu_ji = sum_x[i] / (i + 1)                     if j = 0

7. sum_x_sq[i] - Cumulative Sum of Squares
   Type: numpy.ndarray, shape (n,), dtype float64
   Description: Cumulative sum of squared data up to position i
   Formula:
     sum_x_sq[i] = sum(t=0 to i) (x_t - shift)^2
   Initialization:
     sum_x_sq[0] = (x[0] - shift)^2
     sum_x_sq[i] = sum_x_sq[i-1] + (x[i] - shift)^2 for i > 0
   Purpose: Enables O(1) computation of SSE
   Example:
     If data = [1, 2, 3, 4] and shift = 0
     sum_x_sq = [1, 5, 14, 30]
   Usage: To compute SSE of segment [j, i]:
     SSE(j, i) = sum_x_sq[i] - sum_x_sq[j-1] - (i-j+1) x mu_ji^2  if j > 0
     SSE(0, i) = sum_x_sq[i] - sum_x[i]^2 / (i+1)                if j = 0

8. shift - Numerical Stability Offset
   Type: float
   Description: Constant subtracted from all data points to improve numerical 
                stability
   Default: 0 (can be set to median for better stability)
   Purpose: Reduces floating-point errors when data values are large
   Formula: All computations use (x_t - shift) instead of x_t

   DETAILED EXPLANATION OF SHIFT:
   ------------------------------
   
   What is shift?
   --------------
   Shift is a constant value that is subtracted from every data point before
   computing sums and squared sums. It's a numerical stability technique.
   
   Why use shift?
   -------------
   When data values are large, floating-point arithmetic can accumulate errors:
   - Large numbers have less precision in floating-point representation
   - Subtracting large numbers can cause catastrophic cancellation
   - Squaring large numbers can lead to overflow or precision loss
   
   Example of the problem:
   - Data: [1000000.1, 1000000.2, 1000000.3]
   - Computing variance: need to subtract mean (≈1000000.2) from each value
   - Result: [≈-0.1, ≈0.0, ≈0.1] (small differences between large numbers)
   - Floating-point errors can dominate these small differences
   
   How shift helps:
   --------------
   By subtracting a constant (often the median) from all data points:
   - Data is centered around zero
   - Values become smaller in magnitude
   - Floating-point operations are more accurate
   - Reduces risk of overflow
   
   Mathematical Property (Why Results Don't Change):
   -------------------------------------------------
   The shift doesn't affect the final results because:
   
   1. Mean computation:
      Original: mu = (sum of x_t) / n
      Shifted:  mu' = (sum of (x_t - shift)) / n = mu - shift
      When we add shift back: mu' + shift = mu ✓
   
   2. SSE computation:
      SSE = sum((x_t - mu)^2)
      With shift: SSE' = sum(((x_t - shift) - mu')^2)
                   = sum((x_t - shift - (mu - shift))^2)
                   = sum((x_t - mu)^2) = SSE ✓
   
   The shift cancels out in all final calculations!
   
   Example with shift = 0 (default):
   ----------------------------------
   Data: [1, 2, 3, 4]
   shift = 0
   sum_x = [1, 3, 6, 10]  (cumulative sum of (x_t - 0))
   sum_x_sq = [1, 5, 14, 30]  (cumulative sum of (x_t - 0)^2)
   
   Example with shift = 2.5 (median):
   -----------------------------------
   Data: [1, 2, 3, 4]
   shift = 2.5
   Shifted data: [-1.5, -0.5, 0.5, 1.5]
   sum_x = [-1.5, -2.0, -1.5, 0.0]  (cumulative sum of (x_t - 2.5))
   sum_x_sq = [2.25, 2.5, 2.75, 5.0]  (cumulative sum of (x_t - 2.5)^2)
   
   Computing mean of [0, 3] with shift = 2.5:
   - mu' = sum_x[3] / 4 = 0.0 / 4 = 0.0
   - mu = mu' + shift = 0.0 + 2.5 = 2.5 ✓ (correct: (1+2+3+4)/4 = 2.5)
   
   Computing SSE of [0, 3] with shift = 2.5:
   - SSE' = sum_x_sq[3] - 4 × mu'^2 = 5.0 - 4 × 0 = 5.0
   - But we need to verify: SSE = sum((x_t - 2.5)^2) = 2.25 + 0.25 + 0.25 + 2.25 = 5.0 ✓
   
   When to use non-zero shift:
   ---------------------------
   - Use shift = 0 (default) when data values are small or moderate
   - Use shift = median(data) when data values are very large
   - Use shift = mean(data) as an alternative (though median is often better)
   - The choice doesn't affect correctness, only numerical stability


MATRIX VARIABLES (for Selective Inference)
-------------------------------------------

9. sum_x_matrix[i] - Indicator Vector Matrix
   Type: list of numpy.ndarray, each shape (n, 1)
   Description: sum_x_matrix[i] is an indicator vector where:
                Element j = 1 if j <= i, else 0
   Mathematical Representation:
     sum_x_matrix[i] = [1, 1, ..., 1, 0, ..., 0]^T
                        positions 0 to i are 1, rest are 0
   Initialization:
     sum_x_matrix[0] = [1, 0, 0, ..., 0]^T
     sum_x_matrix[i] = sum_x_matrix[i-1] + e_i where e_i is unit vector
   Purpose: Used to construct constraint matrices for selective inference
   Example: For n = 5, sum_x_matrix[2] = [1, 1, 1, 0, 0]^T

10. sum_x_sq_matrix[i] - Outer Product Matrix
    Type: list of numpy.ndarray, each shape (n, n)
    Description: Outer product of sum_x_matrix[i] with itself
    Formula:
      sum_x_sq_matrix[i] = sum_x_matrix[i] @ sum_x_matrix[i]^T
    Mathematical Representation:
      sum_x_sq_matrix[i][j, k] = 1 if j <= i AND k <= i, else 0
    Purpose: Used in quadratic form computations for constraint matrices
    Example: For n = 3, sum_x_sq_matrix[1] is:
      [[1, 1, 0],
       [1, 1, 0],
       [0, 0, 0]]

    DETAILED EXPLANATION OF sum_x_sq_matrix[i][j, k]:
    -------------------------------------------------
    
    Matrix Structure:
    -----------------
    sum_x_sq_matrix[i] is a 2D matrix of shape (n, n), where:
    - n is the length of the data vector
    - Each element is indexed by [j, k] where:
      * j is the ROW index (0 to n-1)
      * k is the COLUMN index (0 to n-1)
    
    What do j and k represent?
    ---------------------------
    - j and k are both indices into the data vector x = [x_0, x_1, ..., x_{n-1}]
    - j represents a data point position (row)
    - k represents a data point position (column)
    - The matrix element [j, k] encodes whether BOTH positions j and k are 
      within the range [0, i]
    
    The Value at [j, k]:
    --------------------
    sum_x_sq_matrix[i][j, k] = {
        1  if j <= i AND k <= i  (both positions are in range [0, i])
        0  otherwise             (at least one position is beyond i)
    }
    
    Why This Structure?
    -------------------
    This matrix is used to compute quadratic forms like x^T @ M @ x, where:
    - x is the data vector [x_0, x_1, ..., x_{n-1}]
    - M is sum_x_sq_matrix[i]
    - The result: sum_{j=0}^{n-1} sum_{k=0}^{n-1} x_j * M[j,k] * x_k
    - Since M[j,k] = 1 only when j <= i and k <= i, this effectively sums
      all products x_j * x_k for j, k in [0, i]
    
    Concrete Example: n = 3, i = 1
    --------------------------------
    Data vector: x = [x_0, x_1, x_2]
    
    sum_x_matrix[1] = [1, 1, 0]^T  (indicator: positions 0 and 1 are included)
    
    sum_x_sq_matrix[1] = sum_x_matrix[1] @ sum_x_matrix[1]^T
                        = [1, 1, 0]^T @ [1, 1, 0]
                        = [[1, 1, 0],
                           [1, 1, 0],
                           [0, 0, 0]]
    
    Reading the matrix:
    - [0, 0] = 1: position 0 is <= 1 AND position 0 is <= 1 ✓
    - [0, 1] = 1: position 0 is <= 1 AND position 1 is <= 1 ✓
    - [0, 2] = 0: position 0 is <= 1 BUT position 2 is NOT <= 1 ✗
    - [1, 0] = 1: position 1 is <= 1 AND position 0 is <= 1 ✓
    - [1, 1] = 1: position 1 is <= 1 AND position 1 is <= 1 ✓
    - [1, 2] = 0: position 1 is <= 1 BUT position 2 is NOT <= 1 ✗
    - [2, 0] = 0: position 2 is NOT <= 1 ✗
    - [2, 1] = 0: position 2 is NOT <= 1 ✗
    - [2, 2] = 0: position 2 is NOT <= 1 ✗
    
    Visual Representation (n = 3, i = 1):
    --------------------------------------
    Matrix indices:        Data positions:
           k=0  k=1  k=2        0    1    2
    j=0  [  1    1    0  ]  =  [x_0, x_1,  -  ]
    j=1  [  1    1    0  ]     [x_1, x_1,  -  ]
    j=2  [  0    0    0  ]     [  - ,  - ,  - ]
    
    The 1's form a block in the upper-left corner covering positions [0, i]
    
    Example: n = 5, i = 2
    ---------------------
    sum_x_sq_matrix[2] = [[1, 1, 1, 0, 0],
                          [1, 1, 1, 0, 0],
                          [1, 1, 1, 0, 0],
                          [0, 0, 0, 0, 0],
                          [0, 0, 0, 0, 0]]
    
    Pattern: 3x3 block of 1's in upper-left (positions 0, 1, 2)
    
    Usage in Quadratic Forms:
    -------------------------
    When computing x^T @ sum_x_sq_matrix[i] @ x:
    - Only terms x_j * x_k where j <= i and k <= i contribute
    - This is equivalent to: sum_{j=0}^i sum_{k=0}^i x_j * x_k
    - Used to represent cumulative sums of products for selective inference
    
    Connection to sum_x_matrix:
    ---------------------------
    sum_x_sq_matrix[i] is the outer product:
      sum_x_sq_matrix[i] = sum_x_matrix[i] @ sum_x_matrix[i]^T
    
    Where sum_x_matrix[i] is a column vector [1, 1, ..., 1, 0, ..., 0]^T
    with 1's in positions 0 to i.
    
    The outer product creates a matrix where:
    - Each row j corresponds to a data position
    - Each column k corresponds to a data position  
    - The value at [j, k] is the product of the j-th and k-th elements of
      the indicator vector

11. list_matrix[i] - Constraint Matrix Components
    Type: list of numpy.ndarray, each shape (n, n)
    Description: Stores matrix components for constructing selective inference 
                 constraints
    Mathematical Meaning:
      list_matrix[i] represents the quadratic form matrix for optimal 
      segmentation ending at i
      Used to compute: x^T @ list_matrix[i] @ x (quadratic form)
    Initialization:
      list_matrix[i] = ssq_matrix(0, i, ...) for first segment
    Update: During DP, updated when better segmentation is found
    Purpose: Tracks the "shape" of the objective function for selective 
             inference

    DETAILED EXPLANATION OF list_matrix:
    ------------------------------------
    
    What is list_matrix?
    --------------------
    list_matrix is a LIST of matrices (one for each position i = 0, 1, ..., n-1).
    Each list_matrix[i] is a matrix of shape (n, n) that encodes the optimal
    segmentation ending at position i as a quadratic form.
    
    Key Property:
    -------------
    If x is the data vector, then:
      x^T @ list_matrix[i] @ x = Total SSE of optimal segmentation ending at i
    
    This means list_matrix[i] represents the "matrix form" of the objective
    function (sum of squared errors) for the best segmentation found so far
    that ends at position i.
    
    Why Use Matrix Form?
    --------------------
    For selective inference, we need to:
    1. Compare optimal segmentation vs. alternative segmentations
    2. Express these comparisons as constraints: x^T @ A @ x >= 0
    3. Use these constraints to compute valid p-values
    
    The matrix form allows us to:
    - Add matrices to combine segmentations (matrix_Y + matrix_Z)
    - Subtract matrices to compare segmentations (matrix_X - matrix_Y_plus_Z)
    - Express everything as quadratic forms for statistical inference
    
    How list_matrix is Built:
    -------------------------
    
    Initialization (k = 0, single segment):
    ---------------------------------------
    For each i from 0 to n-1:
      list_matrix[i] = ssq_matrix(0, i, ...)
    
    This represents: single segment from 0 to i
    - SSE(0, i) = x^T @ list_matrix[i] @ x
    
    Example: n = 6, i = 2
      list_matrix[2] = matrix for segment [0, 2]
      x^T @ list_matrix[2] @ x = SSE(0, 2)
    
    Update During DP (k >= 1):
    --------------------------
    When computing optimal segmentation ending at i with k+1 segments:
    
    1. Start with candidate: no new changepoint
       - matrix_Y = list_matrix[i-1]  (optimal ending at i-1)
       - This represents: previous segments + empty last segment
    
    2. For each candidate j (start of last segment):
       - matrix_Y = list_matrix[j-1]  (optimal ending at j-1 with k segments)
       - matrix_Z = ssq_matrix(j, i)  (SSE matrix for segment [j, i])
       - matrix_Y_plus_Z = matrix_Y + matrix_Z  (combined segmentation)
       - This represents: optimal k segments ending at j-1 + segment [j, i]
    
    3. If this candidate is better (SSQ_j < S[k][i]):
       - Update: list_matrix[i] = matrix_Y + matrix_Z
       - This stores the matrix for the NEW optimal segmentation
    
    Mathematical Relationship:
    -------------------------
    list_matrix[i] encodes the decomposition:
      Total SSE = sum of SSE for each segment
    
    For a segmentation with segments [0, j₁-1], [j₁, j₂-1], ..., [j_k, i]:
      list_matrix[i] = ssq_matrix(0, j₁-1) + ssq_matrix(j₁, j₂-1) + ... 
                       + ssq_matrix(j_k, i)
    
    This works because:
    - Each ssq_matrix(j, i) represents SSE for one segment
    - Matrix addition combines the segments
    - The quadratic form x^T @ (A + B) @ x = x^T @ A @ x + x^T @ B @ x
    
    Example Walkthrough:
    --------------------
    Data: [1, 1, 1, 5, 5, 5], n = 6, K = 2
    
    Initialization (k = 0):
    - list_matrix[0] = ssq_matrix(0, 0)  (segment [0, 0])
    - list_matrix[1] = ssq_matrix(0, 1)  (segment [0, 1])
    - list_matrix[2] = ssq_matrix(0, 2)  (segment [0, 2])
    - ...
    - list_matrix[5] = ssq_matrix(0, 5)  (segment [0, 5])
    
    k = 1, i = 5 (finding optimal 2-segment solution ending at 5):
    - Try j = 4:
      * matrix_Y = list_matrix[3]  (optimal 1-segment ending at 3)
      * matrix_Z = ssq_matrix(4, 5)  (segment [4, 5])
      * Candidate: matrix_Y + matrix_Z
    
    - Try j = 3:
      * matrix_Y = list_matrix[2]  (optimal 1-segment ending at 2)
      * matrix_Z = ssq_matrix(3, 5)  (segment [3, 5])
      * Candidate: matrix_Y + matrix_Z
      * This is better! Update: list_matrix[5] = matrix_Y + matrix_Z
    
    Final result:
    - list_matrix[5] represents optimal 2-segment solution: [0,2] + [3,5]
    - x^T @ list_matrix[5] @ x = SSE(0,2) + SSE(3,5) = 0 + 0 = 0
    
    Usage in Constraint Generation:
    --------------------------------
    When comparing optimal vs. alternative segmentations:
    
    1. matrix_X = list_matrix[i]  (optimal segmentation ending at i)
    2. matrix_Y_plus_Z = alternative segmentation ending at i
    3. Constraint: matrix_X - matrix_Y_plus_Z
       - This represents: optimal was better than alternative
       - Constraint: x^T @ (matrix_X - matrix_Y_plus_Z) @ x <= 0
       - Which means: SSE(optimal) <= SSE(alternative)
    
    These constraints define the "selection region" - the set of data vectors
    for which the optimal segmentation would be chosen. This is crucial for
    selective inference to compute valid p-values.
    
    Key Insight:
    -----------
    list_matrix[i] is the "memory" of the DP algorithm in matrix form.
    Just as S[k][i] stores the optimal COST (scalar), list_matrix[i] stores
    the optimal COST FUNCTION (matrix) that can be used for:
    - Comparing with alternatives
    - Generating constraints
    - Statistical inference

12. ssq_matrix(j, i, ...) - Segment SSE Matrix
    Type: Function returning numpy.ndarray, shape (n, n)
    Description: Computes the quadratic form matrix for SSE of segment [j, i]
    Mathematical Formula:
      ssq_matrix(j, i) = sum_x_sq_matrix[i] - sum_x_sq_matrix[j-1] 
                         - (i-j+1) x mu_ji_matrix @ mu_ji_matrix^T
      where mu_ji_matrix = (sum_x_matrix[i] - sum_x_matrix[j-1]) / (i-j+1)
    Purpose: Represents SSE as a quadratic form: 
             SSE(j, i) = x^T @ ssq_matrix(j, i) @ x


CONSTRAINT GENERATION VARIABLES
--------------------------------

13. list_condition_matrix - Selective Inference Constraints
    Type: list of numpy.ndarray, each shape (n, n)
    Description: Stores all constraint matrices needed for selective inference
    Mathematical Meaning: Each matrix A represents a constraint:
      x^T @ A @ x >= 0
    Generation: Created during DP when comparing optimal vs. alternative 
                segmentations
    Formula:
      constraint = matrix_X - matrix_Y_plus_Z
      where:
        matrix_X = matrix for optimal segmentation ending at i
        matrix_Y_plus_Z = matrix for alternative segmentation ending at i
    Purpose: These constraints define the "selection region" for valid 
             p-value computation
    Usage: In inference step, constraints determine feasible region for test 
           statistic

14. matrix_X - Optimal Segmentation Matrix
    Type: numpy.ndarray, shape (n, n)
    Description: Quadratic form matrix for the optimal segmentation ending at 
                 position i
    Context: In DP loop, represents best segmentation found so far
    Update: Updated when SSQ_j < S[k][i] (better segmentation found)

15. matrix_Y - Previous Segment Matrix
    Type: numpy.ndarray, shape (n, n)
    Description: Quadratic form matrix from previous DP state
    Context: Represents segmentation ending at j-1 with k-1 segments
    Usage: Combined with matrix_Z to form alternative segmentation

16. matrix_Z - Current Segment Matrix
    Type: numpy.ndarray, shape (n, n)
    Description: Quadratic form matrix for segment [j, i]
    Formula: matrix_Z = ssq_matrix(j, i, ...)
    Usage: Combined with matrix_Y to form alternative segmentation

17. matrix_Y_plus_Z - Alternative Segmentation Matrix
    Type: numpy.ndarray, shape (n, n)
    Description: Quadratic form matrix for alternative segmentation ending at i
    Formula: matrix_Y_plus_Z = matrix_Y + matrix_Z
    Purpose: Represents a non-optimal segmentation that we compare against 
             optimal one
    Usage: Difference matrix_X - matrix_Y_plus_Z becomes a constraint

18. list_matrix_Y_plus_Z - Collection of Alternatives
    Type: list of numpy.ndarray
    Description: Stores all alternative segmentation matrices considered at 
                 position i
    Purpose: Each alternative generates a constraint comparing it to optimal 
             segmentation


BACKTRACKING VARIABLES
----------------------

19. segment_index - Segment Assignment Array
    Type: numpy.ndarray, shape (n,), dtype int64
    Description: For each data point, stores which segment it belongs to
    Values: segment_index[i] in {1, 2, ..., K}
    Example:
      If segment_index = [1, 1, 1, 2, 2, 2, 3, 3]
      Then segments are: [0-2], [3-5], [6-7]
    Usage: Used in inference to identify which segments to compare

20. sg_results - Changepoint Positions
    Type: list of int
    Description: List of changepoint positions (right boundaries of segments)
    Format: [0, cp_1, cp_2, ..., cp_{K-1}, n]
    Example:
      If sg_results = [0, 3, 6, 10] for n=10, K=3
      Changepoints are at positions 3 and 6
      Segments: [0-2], [3-5], [6-9]
    Computation: Built during backtracking using J table

21. segment_left - Left Boundary of Segment
    Type: int
    Description: Starting position (inclusive) of current segment during 
                 backtracking
    Computation: segment_left = J[segment][segment_right]

22. segment_right - Right Boundary of Segment
    Type: int
    Description: Ending position (inclusive) of current segment during 
                 backtracking
    Initialization: segment_right = n - 1 (start from end)
    Update: segment_right = segment_left - 1 (move to previous segment)


LOOP CONTROL VARIABLES
----------------------

23. k - Current Number of Segments (minus 1)
    Type: int
    Range: k in {0, 1, 2, ..., K-1}
    Description: DP iteration counter for number of segments
    Meaning: When k = m, we're computing optimal segmentation with m+1 
             segments
    Example:
      k = 0: Computing single-segment solutions
      k = 1: Computing two-segment solutions
      k = K-1: Computing final K-segment solution

24. i - Current End Position
    Type: int
    Range: i in {imin, imin+1, ..., imax}
    Description: Right boundary of current segment being considered
    Meaning: We're computing S[k][i] = optimal cost ending at position i

25. j - Candidate Start Position
    Type: int
    Range: j in {jmin, jmin+1, ..., i-1}
    Description: Left boundary candidate for last segment
    Meaning: Testing segmentation: [0 to j-1] (k segments) + [j to i] 
             (1 segment)
    Optimization: We find j* that minimizes S[k-1][j-1] + SSE(j, i)

26. imin - Minimum Valid End Position
    Type: int
    Description: Minimum value of i for which S[k][i] is meaningful
    Formula:
      imin = max(1, k)      if k < K-1
      imin = n - 1          if k = K-1
    Reasoning:
      Need at least k+1 data points for k+1 segments
      Last segment must end at n-1 (last data point)

27. imax - Maximum Valid End Position
    Type: int
    Description: Maximum value of i (always n-1)
    Formula: imax = n - 1

28. jmin - Minimum Valid Start Position
    Type: int
    Description: Minimum value of j for valid segmentation
    Formula: jmin = k
    Reasoning: Need at least k segments before position j


HELPER FUNCTION VARIABLES
--------------------------

29. ssq(j, i, sum_x, sum_x_sq) - Sum of Squared Errors
    Type: Function returning float
    Description: Computes SSE for segment [j, i]
    Formula:
      if j > 0:
          mu_ji = (sum_x[i] - sum_x[j-1]) / (i - j + 1)
          SSE = sum_x_sq[i] - sum_x_sq[j-1] - (i-j+1) x mu_ji^2
      else:
          SSE = sum_x_sq[i] - sum_x[i]^2 / (i+1)
    Return: max(0, SSE) (ensures non-negative)
    Purpose: Core computation used in DP recurrence

30. SSQ_j - Total Cost for Candidate Segmentation
    Type: float
    Description: Total SSE for segmentation ending at i with last segment 
                 starting at j
    Formula: SSQ_j = SSE(j, i) + S[k-1][j-1]
    Meaning:
      S[k-1][j-1] = optimal cost for first k segments ending at j-1
      SSE(j, i) = cost of last segment [j, i]
      SSQ_j = total cost of this candidate segmentation
    Usage: Compared with S[k][i] to find minimum


================================================================================
DYNAMIC PROGRAMMING ALGORITHM FLOW
================================================================================

PHASE 1: FORWARD DP (Fill Tables)
---------------------------------

1. Initialize:
   - Compute sum_x and sum_x_sq arrays
   - Initialize S[0][i] = SSE(0, i) for all i
   - Initialize J[0][i] = 0 for all i
   - Initialize list_matrix[i] for all i

2. For k = 1 to K-1:
   a. Determine imin and imax
   b. For i = imin to imax:
      - Initialize S[k][i] = S[k-1][i-1] (no changepoint)
      - Initialize J[k][i] = i
      - For j = i-1 down to k:
         * Compute SSQ_j = S[k-1][j-1] + SSE(j, i)
         * If SSQ_j < S[k][i]:
            - Update S[k][i] = SSQ_j
            - Update J[k][i] = j
            - Update list_matrix[i]
         * Generate constraint: matrix_X - matrix_Y_plus_Z
            - Append to list_condition_matrix


PHASE 2: BACKTRACKING (Recover Changepoints)
---------------------------------------------

1. Initialize:
   - segment_right = n - 1
   - sg_results = []

2. For segment = K-1 down to 0:
   a. segment_left = J[segment][segment_right]
   b. Append (segment_right + 1) to sg_results
   c. Set segment_index[segment_left:segment_right+1] = segment + 1
   d. If segment > 0:
      segment_right = segment_left - 1

3. Append 0 to sg_results
4. Reverse sg_results


================================================================================
TIME AND SPACE COMPLEXITY
================================================================================

TIME COMPLEXITY
---------------
- Precomputation: O(n) for sum arrays
- DP Filling: O(K x n^2) - nested loops over k, i, j
- Constraint Generation: O(K x n^2 x n^2) - matrix operations for each 
                         constraint
- Backtracking: O(K) - linear in number of segments
- Overall: O(K x n^4) (dominated by constraint matrix generation)

SPACE COMPLEXITY
----------------
- DP Tables: O(K x n) for S and J
- Sum Arrays: O(n) for sum_x, sum_x_sq
- Matrix Storage: O(K x n^2 x n^2) for constraint matrices
- Overall: O(K x n^4)


================================================================================
KEY INSIGHTS
================================================================================

1. Optimal Substructure: The optimal segmentation ending at i with k+1 
   segments consists of:
   - Optimal segmentation ending at j-1 with k segments
   - Plus optimal single segment from j to i

2. Constraint Generation: For selective inference, we need to compare:
   - Optimal segmentation (matrix_X)
   - All alternative segmentations (matrix_Y_plus_Z)
   - Difference gives constraint: optimal was better -> constraint satisfied

3. Efficiency: Precomputed sum arrays allow O(1) computation of:
   - Segment means: mu_ji
   - Segment SSE: SSE(j, i)
   - Without precomputation, each would be O(segment_length)

4. Selective Inference: The constraint matrices capture the "selection event" 
   - which segmentation was chosen. This allows computation of valid 
   p-values conditional on the selection.


================================================================================
EXAMPLE WALKTHROUGH
================================================================================

Consider data = [1, 1, 1, 5, 5, 5], n = 6, K = 2 (1 changepoint):

STEP-BY-STEP CALCULATION EXPLANATION:
=====================================

1. COMPUTING sum_x (Cumulative Sum):
   ----------------------------------
   Formula: sum_x[i] = sum(t=0 to i) (x_t - shift), where shift = 0
   
   i=0: sum_x[0] = x[0] - 0 = 1 - 0 = 1
   i=1: sum_x[1] = sum_x[0] + x[1] - 0 = 1 + 1 = 2
   i=2: sum_x[2] = sum_x[1] + x[2] - 0 = 2 + 1 = 3
   i=3: sum_x[3] = sum_x[2] + x[3] - 0 = 3 + 5 = 8
   i=4: sum_x[4] = sum_x[3] + x[4] - 0 = 8 + 5 = 13
   i=5: sum_x[5] = sum_x[4] + x[5] - 0 = 13 + 5 = 18
   
   Result: sum_x = [1, 2, 3, 8, 13, 18]


2. COMPUTING sum_x_sq (Cumulative Sum of Squares):
   -------------------------------------------------
   Formula: sum_x_sq[i] = sum(t=0 to i) (x_t - shift)^2, where shift = 0
   
   i=0: sum_x_sq[0] = (x[0] - 0)^2 = 1^2 = 1
   i=1: sum_x_sq[1] = sum_x_sq[0] + (x[1] - 0)^2 = 1 + 1^2 = 1 + 1 = 2
   i=2: sum_x_sq[2] = sum_x_sq[1] + (x[2] - 0)^2 = 2 + 1^2 = 2 + 1 = 3
   i=3: sum_x_sq[3] = sum_x_sq[2] + (x[3] - 0)^2 = 3 + 5^2 = 3 + 25 = 28
   i=4: sum_x_sq[4] = sum_x_sq[3] + (x[4] - 0)^2 = 28 + 5^2 = 28 + 25 = 53
   i=5: sum_x_sq[5] = sum_x_sq[4] + (x[5] - 0)^2 = 53 + 5^2 = 53 + 25 = 78
   
   Result: sum_x_sq = [1, 2, 3, 28, 53, 78]


3. COMPUTING S[0] (SSE for Single Segments):
   -------------------------------------------
   Formula: S[0][i] = SSE(0, i) = sum_x_sq[i] - sum_x[i]^2 / (i+1)
   
   i=0: S[0][0] = sum_x_sq[0] - sum_x[0]^2 / 1
        = 1 - 1^2 / 1 = 1 - 1 = 0
        (Segment [0,0]: mean = 1, SSE = (1-1)^2 = 0)
   
   i=1: S[0][1] = sum_x_sq[1] - sum_x[1]^2 / 2
        = 2 - 2^2 / 2 = 2 - 4/2 = 2 - 2 = 0
        (Segment [0,1]: mean = (1+1)/2 = 1, SSE = (1-1)^2 + (1-1)^2 = 0)
   
   i=2: S[0][2] = sum_x_sq[2] - sum_x[2]^2 / 3
        = 3 - 3^2 / 3 = 3 - 9/3 = 3 - 3 = 0
        (Segment [0,2]: mean = (1+1+1)/3 = 1, SSE = 0)
   
   i=3: S[0][3] = sum_x_sq[3] - sum_x[3]^2 / 4
        = 28 - 8^2 / 4 = 28 - 64/4 = 28 - 16 = 12
        (Segment [0,3]: mean = (1+1+1+5)/4 = 2, 
         SSE = (1-2)^2 + (1-2)^2 + (1-2)^2 + (5-2)^2 = 1+1+1+9 = 12)
   
   i=4: S[0][4] = sum_x_sq[4] - sum_x[4]^2 / 5
        = 53 - 13^2 / 5 = 53 - 169/5 = 53 - 33.8 = 19.2 ≈ 18
        (Segment [0,4]: mean = (1+1+1+5+5)/5 = 2.6,
         SSE = (1-2.6)^2 + (1-2.6)^2 + (1-2.6)^2 + (5-2.6)^2 + (5-2.6)^2
         = 2.56 + 2.56 + 2.56 + 5.76 + 5.76 = 19.2)
        Note: The value 18 in the example may be rounded or computed slightly
        differently, but the concept is the same.
   
   i=5: S[0][5] = sum_x_sq[5] - sum_x[5]^2 / 6
        = 78 - 18^2 / 6 = 78 - 324/6 = 78 - 54 = 24
        (Segment [0,5]: mean = (1+1+1+5+5+5)/6 = 3,
         SSE = (1-3)^2 + (1-3)^2 + (1-3)^2 + (5-3)^2 + (5-3)^2 + (5-3)^2
         = 4 + 4 + 4 + 4 + 4 + 4 = 24)
   
   Result: S[0] = [0, 0, 0, 12, 18, 24]


4. INITIALIZING J[0] (Backtracking Table for k=0):
   -------------------------------------------------
   For k=0 (single segment), all segments start at position 0.
   Therefore: J[0][i] = 0 for all i
   
   Result: J[0] = [0, 0, 0, 0, 0, 0]


5. DP STEP: k = 1, i = 5 (Finding Optimal 2-Segment Solution):
   -------------------------------------------------------------
   We want to find the optimal 2-segment solution ending at position 5.
   
   Formula: S[1][5] = min_{j=1 to 4} { S[0][j-1] + SSE(j, 5) }
   
   We try each candidate j:
   
   j = 1: Segmentation: [0,0] + [1,5]
         - S[0][0] = 0 (optimal 1-segment ending at 0)
         - SSE(1,5): segment [1,5] = [1, 1, 5, 5, 5]
           * Mean = (1+1+5+5+5)/5 = 17/5 = 3.4
           * SSE = (1-3.4)^2 + (1-3.4)^2 + (5-3.4)^2 + (5-3.4)^2 + (5-3.4)^2
             = 5.76 + 5.76 + 2.56 + 2.56 + 2.56 = 19.2
         - SSQ_1 = 0 + 19.2 = 19.2
   
   j = 2: Segmentation: [0,1] + [2,5]
         - S[0][1] = 0 (optimal 1-segment ending at 1)
         - SSE(2,5): segment [2,5] = [1, 5, 5, 5]
           * Mean = (1+5+5+5)/4 = 16/4 = 4
           * SSE = (1-4)^2 + (5-4)^2 + (5-4)^2 + (5-4)^2
             = 9 + 1 + 1 + 1 = 12
         - SSQ_2 = 0 + 12 = 12
   
   j = 3: Segmentation: [0,2] + [3,5]  ← OPTIMAL!
         - S[0][2] = 0 (optimal 1-segment ending at 2)
         - SSE(3,5): segment [3,5] = [5, 5, 5]
           * Mean = (5+5+5)/3 = 15/3 = 5
           * SSE = (5-5)^2 + (5-5)^2 + (5-5)^2 = 0 + 0 + 0 = 0
         - SSQ_3 = 0 + 0 = 0  ← MINIMUM!
   
   j = 4: Segmentation: [0,3] + [4,5]
         - S[0][3] = 12 (optimal 1-segment ending at 3)
         - SSE(4,5): segment [4,5] = [5, 5]
           * Mean = (5+5)/2 = 5
           * SSE = (5-5)^2 + (5-5)^2 = 0
         - SSQ_4 = 12 + 0 = 12
   
   Best candidate: j = 3 with SSQ_3 = 0
   Update: S[1][5] = 0, J[1][5] = 3
   
   Result: Optimal 2-segment solution ending at 5:
           - Segment 1: [0,2] with SSE = 0
           - Segment 2: [3,5] with SSE = 0
           - Total SSE = 0


6. BACKTRACKING (Recovering Changepoint Positions):
   --------------------------------------------------
   We start from the end and work backwards using the J table.
   
   Step 1: Initialize
   - segment_right = 5 (last position)
   - sg_results = [] (empty list)
   
   Step 2: Recover segments (segment = K-1 = 1 down to 0)
   
   segment = 1 (last segment):
   - segment_left = J[1][5] = 3
   - Last segment: [3, 5]
   - Append (segment_right + 1) = 6 to sg_results → sg_results = [6]
   - segment_right = segment_left - 1 = 3 - 1 = 2 (move to previous segment)
   
   segment = 0 (first segment):
   - segment_left = J[0][2] = 0
   - First segment: [0, 2]
   - Append (segment_right + 1) = 3 to sg_results → sg_results = [6, 3]
   - segment = 0 is the last iteration, so we stop
   
   Step 3: Append 0 and reverse
   - Append 0: sg_results = [6, 3, 0]
   - Reverse: sg_results = [0, 3, 6]
   
   Result: Changepoint positions
   - sg_results = [0, 3, 6]
   - This means: segments are [0-2] and [3-5]
   - Changepoint is at position 3 (between positions 2 and 3)
   
   Final Segmentation:
   - Segment 1: positions [0, 1, 2] = [1, 1, 1] with mean = 1
   - Segment 2: positions [3, 4, 5] = [5, 5, 5] with mean = 5
   - Changepoint: between position 2 and 3


================================================================================
REFERENCES
================================================================================

Paper: "Computing Valid p-value for Optimal Changepoint by Selective Inference 
        using Dynamic Programming" (NeurIPS 2020)
arXiv: https://arxiv.org/abs/2002.09132

